\section{Reinforcement Learning in der Quantitativen Finanzwissenschaft}
\label{sec:rl_finance}

Die Anwendung von \gls{reinforcement_learning} in den Finanzmärkten hat in den letzten Jahren erhebliche Fortschritte gemacht. Aufgrund der stochastischen Natur der Märkte, der hohen Dimensionalität von Handelsstrategien und der Notwendigkeit adaptiver Entscheidungsfindung ist RL eine vielversprechende Methode zur Optimierung algorithmischer Handelsstrategien und Portfoliomanagement-Techniken\footcite{Moody1998, Dempster2006}.

\subsection{Grundlagen des Reinforcement Learning}
Das Konzept von \gls{reinforcement_learning} basiert auf einem Agenten, der in einer Umgebung agiert und durch Belohnungen lernt, welche Handlungen optimal sind. Formell wird dies durch einen \gls{markov_decision_process} (MDP) beschrieben, der aus einem Tupel \( (S, A, P, R, \gamma) \) besteht:

\begin{itemize}
	\setlength\itemsep{0.1em}
	\item \( S \) - die Menge der möglichen Zustände der Umwelt,
	\item \( A \) - die Menge der möglichen Aktionen des Agenten,
	\item \( P(s'|s,a) \) - die Übergangswahrscheinlichkeit vom Zustand \( s \) zu \( s' \) nach Aktion \( a \),
	\item \( R(s,a) \) - die erhaltene Belohnung nach Ausführen der Aktion \( a \) in Zustand \( s \),
	\item \( \gamma \in [0,1] \) - der Diskontierungsfaktor, der zukünftige Belohnungen gewichtet.
\end{itemize}

Das Ziel des Agenten ist es, eine optimale \gls{policy} \( \pi(a|s) \) zu finden, die den erwarteten kumulierten Nutzen maximiert:

\begin{equation}
	V^\pi(s) = \mathbb{E} \left[ \sum_{t=0}^{\infty} \gamma^t R(s_t, a_t) \mid s_0 = s \right].
\end{equation}

\subsection{Value-Based und Policy-Based Methoden}
Innerhalb des RL gibt es zwei Hauptansätze zur Optimierung der Entscheidungsstrategie:

\subsubsection{Value-Based Methoden}
Hierbei wird die sogenannte **Q-Funktion** geschätzt, die den erwarteten Nutzen eines Zustands-Aktions-Paares beschreibt:

\begin{equation}
	Q(s,a) = \mathbb{E} \left[ \sum_{t=0}^{\infty} \gamma^t R(s_t, a_t) \mid s_0 = s, a_0 = a \right].
\end{equation}

Zu den bekanntesten Methoden gehören:
\begin{itemize}
	\item **Q-Learning**: Eine tabellarische Methode zur iterativen Approximation der Q-Werte\footcite{Watkins1992}.
	\item **Deep Q-Networks (DQN)**: Erweiterung von Q-Learning durch neuronale Netze zur Approximation von Q-Werten bei hochdimensionalen Zustandsräumen\footcite{Mnih2015}.
\end{itemize}

\subsubsection{Policy-Based Methoden}
Policy-Gradient-Methoden optimieren direkt die Policy \( \pi(a|s) \), anstatt Wertefunktionen zu approximieren. Wichtige Algorithmen sind:

\begin{itemize}
	\item **REINFORCE**: Ein einfacher Policy-Gradient-Ansatz, der die Strategie direkt aktualisiert\footcite{Williams1992}.
	\item **Actor-Critic-Methoden**: Kombination aus Policy-Optimierung und Wertfunktion-Schätzung zur Stabilisierung des Lernprozesses\footcite{Sutton2000}.
\end{itemize}

\subsection{Deep Reinforcement Learning und Finanzmärkte}
Durch den Einsatz neuronaler Netze hat sich **Deep Reinforcement Learning (DRL)** als leistungsfähige Technik zur Modellierung von Finanzmärkten etabliert. Insbesondere in Handelsstrategien, Portfoliomanagement und Market Making wurden DRL-Methoden erfolgreich angewendet\footcite{Dixon2020, Zhang2023}.

Zu den prominenten DRL-Ansätzen gehören:
\begin{itemize}
	\item **Deep Deterministic Policy Gradient (DDPG)**: Ermöglicht kontinuierliche Aktionsräume für quantitative Strategien\footcite{Lillicrap2016}.
	\item **Proximal Policy Optimization (PPO)**: Verbessert die Stabilität und Effizienz des Lernprozesses im Vergleich zu traditionellen Policy-Gradient-Methoden\footcite{Schulman2017}.
\end{itemize}

\subsection{Vergleich mit traditionellen Optimierungsmethoden}
Traditionelle Finanzmodelle basieren oft auf **heuristischen oder regelbasierten Strategien**, die nicht adaptiv auf sich ändernde Marktbedingungen reagieren. RL-Ansätze bieten im Vergleich dazu folgende Vorteile:

\begin{itemize}
	\item \textbf{Anpassungsfähigkeit}: RL kann sich an veränderte Marktbedingungen anpassen, während klassische Modelle oft eine stationäre Umgebung voraussetzen.
	\item \textbf{Automatisierte Entscheidungsfindung}: RL-Algorithmen optimieren Strategien direkt aus historischen und simulierten Marktdaten.
	\item \textbf{Generalität}: RL kann auf verschiedene Handelsstrategien angewendet werden, von Market Making bis hin zu Risikomanagement\footcite{Fischer2018}.
\end{itemize}

\subsection{Kritische Betrachtung der Anwendbarkeit im Finanzsektor}
Trotz der Fortschritte von RL gibt es mehrere Herausforderungen für den praktischen Einsatz im Finanzsektor:

\begin{itemize}
	\item \textbf{Overfitting}: RL-Modelle können an vergangene Marktdaten überangepasst sein und schlecht auf neue Marktsituationen generalisieren.
	\item \textbf{Datenqualität und Verfügbarkeit}: Historische Daten sind oft begrenzt oder durch **Survivorship Bias** beeinflusst.
	\item \textbf{Regulatorische Unsicherheiten}: Der Einsatz von KI-basierten Algorithmen im Finanzhandel unterliegt zunehmend regulatorischen Beschränkungen\footcite{Alderman2021}.
\end{itemize}
